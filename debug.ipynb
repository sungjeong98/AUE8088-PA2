{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6MPjfT5NrKQ"
      },
      "source": [
        "# Jupyter notebook for debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvMlHd_QwMG",
        "outputId": "e8225db4-e61d-4640-8b1f-8bfce3331cea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# Copied from `train` function in train_simple.py:L78\n",
        "import yaml\n",
        "\n",
        "device = 'cpu'\n",
        "hyp = 'data/hyps/hyp.scratch-low.yaml'\n",
        "\n",
        "with open(hyp, errors=\"ignore\") as f:\n",
        "    hyp = yaml.safe_load(f)  # load hyps dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overriding model.yaml nc=4 with nc=15\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]              \n",
            "  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]                \n",
            "  2                -1  1      4800  models.common.C3                        [32, 32, 1]                   \n",
            "  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  4                -1  2     29184  models.common.C3                        [64, 64, 2]                   \n",
            "  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  6                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  8                -1  1    296448  models.common.C3                        [256, 256, 1]                 \n",
            "  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]                 \n",
            " 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]           \n",
            " 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]          \n",
            " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 24      [17, 20, 23]  1     18040  models.yolo.Detect                      [15, [[10.34, 11.59, 30.87, 22.7], [37.16, 78.79, 68.29, 35.23], [74.31, 182.29, 124.75, 80.0]], [64, 128, 256]]\n",
            "YOLOv5n_nuscenes summary: 214 layers, 1775192 parameters, 1775192 gradients, 4.3 GFLOPs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from models.yolo import Model\n",
        "from utils.general import check_dataset\n",
        "\n",
        "cfg = 'models/yolov5n_nuscenes.yaml'\n",
        "data = 'data/nuscenes.yaml'\n",
        "data_dict = check_dataset(data)\n",
        "\n",
        "nc = int(data_dict[\"nc\"])  # number of classes\n",
        "model = Model(cfg, ch=3, nc=nc, anchors=hyp.get(\"anchors\")).to(device)  # create"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "anchors = model.model[-1].anchors\n",
        "nl = model.model[-1].nl\n",
        "\n",
        "\n",
        "# [TODO] Draw anchors\n",
        "# Code below data loader cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/sung/nuscenes_det2d/train.cache... 28130 images, 1425 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28130/28130 [00:00<?, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "from utils.dataloaders import create_dataloader\n",
        "from utils.general import check_img_size, colorstr\n",
        "\n",
        "imgsz = 416\n",
        "batch_size = 1\n",
        "single_cls = False\n",
        "seed = 0\n",
        "\n",
        "train_path = data_dict[\"train\"]\n",
        "gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
        "imgsz = check_img_size(imgsz, gs, floor=gs * 2)  # verify imgsz is gs-multiple\n",
        "\n",
        "train_loader, dataset = create_dataloader(\n",
        "    train_path,\n",
        "    imgsz,\n",
        "    batch_size,\n",
        "    gs,\n",
        "    single_cls,\n",
        "    hyp=hyp,\n",
        "    augment=True,\n",
        "    cache=None,\n",
        "    rect=False,\n",
        "    rank=-1,\n",
        "    workers=8,\n",
        "    image_weights=False,\n",
        "    quad=False,\n",
        "    prefix=colorstr(\"train: \"),\n",
        "    shuffle=True,\n",
        "    seed=seed,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/sung/nuscenes_det2d/train.cache... 28130 images, 1425 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28130/28130 [00:00<?, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "object_sizes = []\n",
        "\n",
        "for imgs, targets, paths, _ in train_loader:\n",
        "    imgs = imgs.to(device, non_blocking=True).float() / 255  # uint8 to float32, 0-255 to 0.0-1.0\n",
        "    break\n",
        "\n",
        "#accumulating width and height of targets to analyze distribution\n",
        "batch_size = 16\n",
        "train_loader, _ = create_dataloader(\n",
        "    train_path,\n",
        "    imgsz,\n",
        "    batch_size,\n",
        "    gs,\n",
        "    single_cls,\n",
        "    hyp=hyp,\n",
        "    augment=True,\n",
        "    cache=None,\n",
        "    rect=False,\n",
        "    rank=-1,\n",
        "    workers=8,\n",
        "    image_weights=False,\n",
        "    quad=False,\n",
        "    prefix=colorstr(\"train: \"),\n",
        "    shuffle=True,\n",
        "    seed=seed,\n",
        ")\n",
        "\n",
        "#Applying KMeans clustering to find centroids of all object sizes\n",
        "\n",
        "# for _, targets, _, _ in train_loader:\n",
        "#     object_sizes.extend(targets[:, 4:6].numpy())\n",
        "\n",
        "# object_sizes = np.array(object_sizes)\n",
        "\n",
        "# kmeans = KMeans(n_clusters=6)\n",
        "# kmeans.fit(object_sizes)\n",
        "\n",
        "# centroids = kmeans.cluster_centers_\n",
        "\n",
        "# plt.figure(figsize=(10, 6))\n",
        "\n",
        "# plt.scatter(object_sizes[:, 0], object_sizes[:, 1], s=10, c='blue', label='Object Sizes')\n",
        "\n",
        "# plt.scatter(centroids[:, 0], centroids[:, 1], s=100, c='red', marker='x', label='Centroids')\n",
        "\n",
        "# plt.xlabel('Width (w)')\n",
        "# plt.ylabel('Height (h)')\n",
        "# plt.title('Object Sizes')\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "\n",
        "# plt.savefig('object_sizes_and_centroids.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sung/anaconda3/envs/pa2/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            "/home/sung/anaconda3/envs/pa2/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n",
            "/home/sung/anaconda3/envs/pa2/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n"
          ]
        }
      ],
      "source": [
        "#Applying KMeans clustering to find centroids of small, mid and big object sizes\n",
        "\n",
        "small_objects = []\n",
        "mid_objects = []\n",
        "big_objects = []\n",
        "\n",
        "for _, targets, _, _ in train_loader:\n",
        "    for target in targets.numpy():\n",
        "        width, height = target[4:6]\n",
        "        if width <= 0.2 and height <= 0.2:\n",
        "            small_objects.append(target[4:6])\n",
        "        elif width <= 0.4 and height <= 0.4:\n",
        "            mid_objects.append(target[4:6])\n",
        "        else:\n",
        "            big_objects.append(target[4:6])\n",
        "\n",
        "def process_and_plot(objects, category):\n",
        "    if len(objects) > 0:\n",
        "        objects_array = np.array(objects)\n",
        "        kmeans = KMeans(n_clusters=2)\n",
        "        kmeans.fit(objects_array)\n",
        "        centroids = kmeans.cluster_centers_\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(objects_array[:, 0], objects_array[:, 1], s=10, c='blue', label=f'{category} Object Sizes')\n",
        "        plt.scatter(centroids[:, 0], centroids[:, 1], s=100, c='red', marker='x', label='Centroids')\n",
        "        plt.xlabel('Width (w)')\n",
        "        plt.ylabel('Height (h)')\n",
        "        plt.title(f'{category} Object Sizes and Centroids')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(f'{category.lower()}_sizes_and_centroids.png')\n",
        "        plt.show()\n",
        "\n",
        "        return centroids\n",
        "    else:\n",
        "        print(f\"No data available for {category}\")\n",
        "        return []\n",
        "\n",
        "centroids_small = process_and_plot(small_objects, \"Small Objects\")\n",
        "centroids_mid = process_and_plot(mid_objects, \"Mid Objects\")\n",
        "centroids_big_1 = process_and_plot(big_objects, \"Big Objects\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[    0.11045     0.08159]\n",
            " [   0.032083    0.032089]] \n",
            " [[    0.25947     0.13957]\n",
            " [    0.12632      0.2757]] \n",
            " [[    0.47564     0.26189]\n",
            " [    0.17616     0.51576]]\n"
          ]
        }
      ],
      "source": [
        "print(centroids_small,'\\n', centroids_mid, '\\n', centroids_big_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 ðŸš€ ddf4e91 Python-3.8.19 torch-2.3.1+cu121 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients, 4.5 GFLOPs\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from models.common import DetectMultiBackend\n",
        "from utils.torch_utils import select_device\n",
        "\n",
        "weights = 'yolov5n.pt'\n",
        "# data = 'data/nuscenes.yaml'\n",
        "data = 'data/coco128.yaml'\n",
        "half = False  # use FP16 half-precision inference\n",
        "dnn = False  # use OpenCV DNN for ONNX inference\n",
        "device = select_device('cpu')\n",
        "\n",
        "model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
        "\n",
        "# inference\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred = model(imgs)  # forward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Drawing anchor box and grid cells on the image\n",
        "#divide the image into grid cells\n",
        "for head_idx in range(3):\n",
        "    grid_ratio = pred[1][head_idx].shape[2]\n",
        "    grid_size = imgsz // grid_ratio\n",
        "    xy_pos = []\n",
        "    for j in range(grid_ratio):\n",
        "        for k in range(grid_ratio):\n",
        "            xy_pos.append([j * grid_size, k * grid_size])\n",
        "\n",
        "    # plt visualize image with grid cells\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(imgs[0].permute(1,2,0).cpu().numpy())\n",
        "    for idx, (x, y) in enumerate(xy_pos):\n",
        "        ax.add_patch(plt.Rectangle((x, y), grid_size, grid_size, fill=False, edgecolor='red', lw=0.5))\n",
        "        if idx == 135:\n",
        "            center_x, center_y = x + grid_size // 2, y + grid_size // 2\n",
        "            for i in range(len(anchors[head_idx])):\n",
        "                anchor_width = anchors[head_idx][i][0] * grid_size\n",
        "                anchor_height = anchors[head_idx][i][1] * grid_size\n",
        "                ax.add_patch(plt.Rectangle((center_x - anchor_width // 2, center_y - anchor_height // 2), anchor_width, anchor_height, fill=False, edgecolor='blue', lw=0.5))\n",
        "        \n",
        "    plt.show()\n",
        "    #save image with grid cells\n",
        "    fig.savefig('grid_cells_{}.png'.format(grid_ratio), dpi=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.general import non_max_suppression\n",
        "\n",
        "conf_thres = 0.25  # confidence threshold\n",
        "iou_thres = 0.45  # NMS IOU threshold\n",
        "max_det = 1000  # maximum detections per image\n",
        "classes = None\n",
        "agnostic_nms = False  # class-agnostic NMS\n",
        "\n",
        "pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "\n",
        "# [TODO] draw predictions (see detect.py:L178)\n",
        "\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "class_names = [\n",
        "    'pedestrian', 'animal', 'car'\n",
        "]\n",
        "#omitted other classes for simplicity\n",
        "\n",
        "for img, output in zip(imgs, pred):  # Loop through images and corresponding outputs\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(img.permute(1, 2, 0).cpu().numpy())  # Convert tensor image to numpy and display\n",
        "    \n",
        "    if output is not None and len(output) > 0:\n",
        "        for x1, y1, x2, y2, conf, cls_pred in output:\n",
        "            box = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')\n",
        "            ax.add_patch(box)\n",
        "            label = f'{class_names[int(cls_pred)]} {conf:.2f}'\n",
        "            ax.text(x1, y1-13, label, color='white', ha='left', va='top', bbox=dict(facecolor='red', alpha=0.5), fontsize=8)\n",
        "    \n",
        "    plt.axis('off')\n",
        "    plt.savefig('detection.png')  # Save the figure to file\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save video of detected image results\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def images_to_video(image_folder, video_name, fps):\n",
        "    images = [img for img in os.listdir(image_folder) if img.endswith(\".png\") or img.endswith(\".jpg\")]\n",
        "    images.sort()\n",
        "\n",
        "    frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
        "    height, width, layers = frame.shape\n",
        "\n",
        "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'DIVX'), fps, (width, height))\n",
        "\n",
        "    for image in images:\n",
        "        video.write(cv2.imread(os.path.join(image_folder, image)))\n",
        "\n",
        "    video.release()\n",
        "\n",
        "image_folder = 'runs/detect/exp3'\n",
        "video_name = 'output_video.avi'\n",
        "fps = 7\n",
        "\n",
        "images_to_video(image_folder, video_name, fps)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "YOLOv5 Tutorial",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
